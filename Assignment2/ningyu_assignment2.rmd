```{r}
library(dplyr)
library(rvest)
```

# Assignment 2 – Programming Structure and Function

### Q1

Suppose we have a matrix of 1s and 0s. We want to create a vector as follows: for each row of the matrix, the corresponding element of the vector will be either 1 or 0, depending on whether the majority of the first c elements in the row is 1 or 0. Here c will be a parameter which we want to control. Create a function to perform this task. 

```{r}
q1 = function(mtx, c) {
    means = df %>% apply(1, function(x) mean(x[1:c]))
    return((means + 0.5) %>% floor)
}
```

```{r}
df = data.frame(hey=c(1, 0, 1, 0, 0), bye=c(1,1,1,1,0), whee=c(1, 0, 1, 1, 0), la=c(0,1,0,0,0), ah=c(0,0,0,0,0))
```

```{r}
for (i in 1:5) {
    print(q1(df, i))
}
```

### Q2. 

Create a script to crawl all property data from SRX.com.sg. Please show me your codes
(using for loop or while loop) to crawl the complete data, but just need to store two pages
of data into a data frame and save it as “srx.csv”.

```{r}
host_url = 'https://www.srx.com.sg'
```

```{r}
fetch_property_nodes = function(page) {
    url = paste(host_url, "/search/sale/residential?page=", page, sep = "")
    srx_html = read_html(url)
    listing = html_nodes(srx_html, ".listingDetailTitle")
    if(length(listing) == 0)
        return(c('Done'))
#     a<-html_children(listing[1])
#     title <- html_text(a)
#     return (title)
    return(listing)
}
```

```{r}
#Run Result(1000), Result(10000), Result(100000) and compare the results.
trycatch_fetch_property_notes = function(n) {
    result = tryCatch({
        return (fetch_property_nodes(n))
    }, warning = function(n) {
        print(paste("Warning in crawling the ", n, "th page of SRX", sep = ""))
    }, error = function(n) {
        print(paste("Error in crawling the ", n, "th page of SRX", sep = ""))
        return(c('Done'))
    }, finally = function(n) { 
    })
    return (result)
}
```

```{r}
get_df_from_node_list = function(listing_info_node, facilities_info_node, agent_name) {
    labels = listing_info_node %>% html_children %>% `[`(c(T, F)) %>% html_text %>% takeout_last_char
    values = listing_info_node %>% html_children %>% `[`(c(F, T)) %>% html_text %>% remove_tabs
    facilities = facilities_info %>% html_children %>% html_text %>% remove_tabs %>% paste(collapse=' | ')
    
    data_list = list()
    data_list[labels] = values
    data_list['Facilities'] = facilities
    data_list['Agent'] = agent_name
    
    data_list %>% as.data.frame
}
```

```{r}
remove_tabs = function(x) {
    gsub("([\t]|[\r\n])", "", x)
}
```

```{r}
takeout_last_char = function(x) {
    x %>% substr(1, nchar(x) - 1)
}
```

```{r}
#To save time, I started from the 1650th page. You may change it to n<-1L to start from page 1.
n = 1L
prop_df = data.frame()

while(n<3){
    property = trycatch_fetch_property_notes(n)
#     print(property)
    
    if (property[1] == 'Done') {
        break
    }
    
    for (i in 1:length(property)) {
        listing_url = paste(host_url, html_attr(property[i], 'href'), sep="")
        listing_html = read_html(listing_url)
        listing_data = html_nodes(listing_html, '.listing-about-main')
        listing_info = html_nodes(listing_data[1], 'p')
        facilities_info = listing_data[2]
        agent_name = listing_html %>% html_node('.featuredAgentName') %>% html_text
#         if (i == 1) print(agent_name)
        row_df = get_df_from_node_list(listing_info, facilities_info, agent_name)
        prop_df = suppressWarnings(bind_rows(prop_df, row_df))
    }
    
    n = n + 1
}
```

```{r}
prop_df %>% head
```

```{r}
prop_df %>% write.csv(file='srx.csv')
```